{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89c04e2f-61d7-4067-b3b6-ee23db9b1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c809198a-a0e8-4940-8d3b-b0abbfeb77a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a73351e-f8d1-4c9f-a558-044a04dff457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_periods(strings):\n",
    "    return [f\".{s}\" for s in strings]\n",
    "\n",
    "words = add_periods(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c51faede-92a2-497b-a58c-609c4087f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "emma.oliv\n"
     ]
    }
   ],
   "source": [
    "combined_string = \"\".join(words)\n",
    "unique_characters = set(combined_string)\n",
    "sorted_unique_characters = sorted(unique_characters)\n",
    "print(sorted_unique_characters)\n",
    "print(combined_string[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee60b758-1a4f-48e9-91ca-d19d5f03886a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 5, 13, 13,  ..., 26, 24,  0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, concatenated_str, char_list):\n",
    "        self.char_list = char_list\n",
    "        self.feature_tensor = self.create_feature_tensor(concatenated_str, char_list)\n",
    "        self.index_tensor = self.create_index_tensor(concatenated_str, char_list)\n",
    "\n",
    "    def create_feature_tensor(self, concatenated_str, char_list):\n",
    "        feature_tensor = torch.zeros((len(concatenated_str), len(char_list)))\n",
    "        for i, char in enumerate(concatenated_str):\n",
    "            index = char_list.index(char)\n",
    "            feature_tensor[i, index] = 1\n",
    "        return feature_tensor\n",
    "\n",
    "    def create_index_tensor(self, concatenated_str, char_list):\n",
    "        index_tensor = torch.zeros(len(concatenated_str), dtype=torch.long)\n",
    "        for i, char in enumerate(concatenated_str):\n",
    "            index = char_list.index(char)\n",
    "            index_tensor[i] = index\n",
    "\n",
    "        index_tensor = torch.cat((index_tensor[1:], index_tensor[:1]))\n",
    "        return index_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.feature_tensor[idx], self.index_tensor[idx]\n",
    "\n",
    "\n",
    "# Create custom dataset\n",
    "custom_dataset = CustomDataset(combined_string, sorted_unique_characters)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 114073\n",
    "train_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8fc67214-6c82-4eaf-bd29-333eebc6527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7acf1fb-eb0c-44f1-8a0e-2b9d6583c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(27, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 140),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(140, 27),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac250d39-88cc-4d05-9189-c471bcd00076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=27, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=256, out_features=140, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=140, out_features=27, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e9541f6-136e-4631-8f32-1ace68299f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.305241  [114073/228146]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.304935  [114073/228146]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.304895  [114073/228146]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.304738  [114073/228146]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.304768  [114073/228146]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 3.304602  [114073/228146]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8dafa99-45fe-4072-962a-46c51f483e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "input tensor tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "Current Char: w, Predicted Char: w\n",
      "Output logits: [-0.05942978 -0.0369483   0.00131079  0.02187892  0.03135263  0.06781182\n",
      "  0.03764792 -0.01941245 -0.0453702   0.07288419 -0.00994769 -0.07041042\n",
      " -0.05054032  0.06567802 -0.03753499 -0.09150653 -0.0535771   0.07674101\n",
      "  0.05886476 -0.07239634  0.01739429  0.05564881 -0.05542457  0.09783038\n",
      " -0.07530768 -0.05955213 -0.03390758]\n",
      "Generated sequence starting with 'w': wwwwwwwwwwwwwwwwwww\n",
      "Generated word starting with 'w': wwwwwwwwwwwwwwwwwww\n"
     ]
    }
   ],
   "source": [
    "def generate_words(start_char, model, char_list, max_length=20):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_tensor = torch.zeros(1, len(char_list)).to(device)\n",
    "        start_index = char_list.index(start_char)\n",
    "        input_tensor[0, start_index] = 1\n",
    "\n",
    "        generated_words = []\n",
    "        current_char = start_char\n",
    "        for _ in range(max_length):\n",
    "            output = model(input_tensor)\n",
    "            _, predicted_index = torch.max(output, dim=1)\n",
    "            predicted_char = char_list[predicted_index.item()]\n",
    "\n",
    "            generated_words.append(predicted_char)\n",
    "            print(f'input tensor {input_tensor}')\n",
    "            \n",
    "\n",
    "            # Update input tensor for the next iteration\n",
    "            input_tensor.zero_()\n",
    "            input_tensor[0, predicted_index.item()] = 1\n",
    "            \n",
    "\n",
    "            # Debug prints\n",
    "            print(f\"Current Char: {current_char}, Predicted Char: {predicted_char}\")\n",
    "            print(f\"Output logits: {output.squeeze().cpu().numpy()}\")  # Convert to numpy for better readability\n",
    "            if predicted_char == '.':\n",
    "                break\n",
    "\n",
    "\n",
    "        generated_word = ''.join(generated_words[:-1])  # Exclude the last '.'\n",
    "        print(f\"Generated sequence starting with '{start_char}': {generated_word}\")\n",
    "\n",
    "    return generated_word\n",
    "\n",
    "# Example usage:\n",
    "start_char = '.'  # Change start character here\n",
    "generated_word = generate_words(start_char, model, sorted_unique_characters)\n",
    "print(f\"Generated word starting with '{start_char}': {generated_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a627a-64dc-44af-aca6-f2d79dbea58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d86db2-a16e-4847-887b-2f47d20ba54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
